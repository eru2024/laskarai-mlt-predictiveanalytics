# -*- coding: utf-8 -*-
"""Copy of [Predictive Analytics] Submission 1_MLT_Fatih El Haq

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y9qjghn4LMSyRBt91RuBGvn4kWdwS4UK

# Data Understanding

## Import Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns
import pycountry_convert as pc
from IPython.display import Image
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

"""## Data Loading

Dataset: [Hotel booking demand](https://www.kaggle.com/datasets/jessemostipak/hotel-booking-demand/data)

URL dataset: [Github](https://raw.githubusercontent.com/eru2024/laskarai-mlt-predictiveanalytics/refs/heads/main/dataset/hotel_bookings.csv)
"""

# Load dataset from personal github repository
url = 'https://raw.githubusercontent.com/eru2024/laskarai-mlt-predictiveanalytics/refs/heads/main/dataset/hotel_bookings.csv'
hotel_bookings = pd.read_csv(url)
hotel_bookings.head(3)

"""## Dataset Structure"""

# Custom function for summarize dataset structure
def get_dataframe_summary(dfs):
  summary_data = []

  for df_name, df in dfs.items():
    if not isinstance(df, pd.DataFrame):
      print(f"Warning: {df_name} is not a pandas DataFrame. Skipping.")
      continue

    for col_name in df.columns:
      summary_data.append({
          'DataFrame Name': df_name,
          'Column Name': col_name,
          'dtype': df[col_name].dtype,
          'Minimum Value': df[col_name].min() if pd.api.types.is_numeric_dtype(df[col_name]) else np.nan,
          'Maximum Value': df[col_name].max() if pd.api.types.is_numeric_dtype(df[col_name]) else np.nan,
          'Mean Value': df[col_name].mean() if pd.api.types.is_numeric_dtype(df[col_name]) else np.nan,
          'Median Value': df[col_name].median() if pd.api.types.is_numeric_dtype(df[col_name]) else np.nan,
          'Standard Deviation': df[col_name].std() if pd.api.types.is_numeric_dtype(df[col_name]) else np.nan,
          'Number of Rows': len(df),
          'Number of Missing Values': df[col_name].isnull().sum(),
          'Number of Unique Values': df[col_name].nunique(),
          'Number of Duplicated Values': df.duplicated(subset=[col_name]).sum()
      })

  return pd.DataFrame(summary_data)

# Apply function on avalaible dataframes
dataframes = {'hotel_bookings': hotel_bookings}

summary_dataset = get_dataframe_summary(dataframes)
summary_dataset

"""The dataset `hotel_bookings` contains 119,390 rows and 32 columns. Each row represents a single hotel booking. The dataset includes a mix of data types: `int64`, `float64`, and `object` (which represent strings or categorical data).

The presence of various data types indicates that the dataset captures different aspects of a booking, from numerical values like lead time and number of guests to categorical information like hotel type, market segment, and reservation status.

# Exploratory Data Analysis

## Variable Description

Based on the journal article ["Hotel booking demand datasets"](https://doi.org/10.1016/j.dib.2018.11.126) as the dataset original source, there are 32 variables or features with discription below:

1. `hotel`. Categorical, Type of hotel (City Hotel or Resort Hotel)
2. `is_canceled`. Integer, Value indicating if the booking was canceled (1) or not (0)
3. `lead_time`. Integer, Number of days between booking date and arrival date
4. `arrival_date_year`. Integer, Year of arrival date
5. `arrival_date_month`. Categorical, Month of arrival date (January to December)
6. `arrival_date_week_number`. Integer, Week number of arrival date
7. `arrival_date_day_of_month`. Integer, Day of arrival date
8. `stays_in_weekend_nights`. Integer, Number of weekend nights (Saturday or Sunday) the customer stayed or booked to stay at the hotel
9. `stays_in_week_nights`. Integer, Number of week nights (Monday to Friday) the customer stayed or booked to stay at the hotel
10. `adults`. Integer, Number of adults
11. `children`. Integer, Number of children
12. `babies`. Integer, Number of babies
13. `meal`. Categorical, Type of meal booked. Categories are Undefined, BB (Bed & Breakfast), HB (Half Board), FB (Full Board)
14. `country`. Categorical, Country of origin. Categories are represented in the ISO 3166 – 1 alpha-3 format
15. `market_segment`. Categorical, Market segment designation. In categories such as Online TA, Offline TA/TO, Groups, Corporate, Complementary, Aviation
16. `distribution_channel`. Categorical, Booking distribution channel. Categories are Corporate, GDS, TA/TO, Direct, Undefined
17. `is_repeated_guest`. Integer, Value indicating if the booking person is a repeated guest (1) or not (0)
18. `previous_cancellations`. Integer, Number of previous bookings that were canceled by the customer prior to the current booking
19. `previous_bookings_not_canceled`. Integer, Number of previous bookings that were not canceled by the customer prior to the current booking
20. `reserved_room_type`. Categorical, Code of room type reserved. Codes are for illustration purposes only.
21. `assigned_room_type`. Categorical, Code of room type assigned. Codes are for illustration purposes only.
22. `booking_changes`. Integer, Number of changes/amendments made to the booking from the moment it was entered on the PMS until the moment of check-in or cancellation
23. `deposit_type`. Categorical, Indication on if the customer made a deposit to guarantee the booking. Categories are No Deposit, Non Refund, Refundable
24. `agent`. Categorical, ID of the travel agency that made the booking
25. `company`. Categorical, ID of the company/entity that made the booking or responsible for paying the booking
26. `days_in_waiting_list`. Integer, Number of days the booking was in the waiting list before it was confirmed to the customer
27. `customer_type`. Categorical, Type of customer, assuming one of four categories: Contract, Group, Transient, Transient-Party
28. `adr`. Numeric, Average Daily Rate as defined by dividing the sum of all lodging transactions by the total number of staying nights
29. `required_car_parking_spaces`. Integer, Number of required car parking spaces
30. `total_of_special_requests`. Integer, Number of special requests made by the customer (e.g. twin bed or high floor)
31. `reservation_status`. Categorical, Reservation status (Canceled, Check-Out, No-Show)
32. `reservation_status_date`. Date, Date at which the last status was set. This variable can be used in conjunction with the `is_canceled` variable to understand when a booking was really canceled.
"""

hotel_bookings['reservation_status_date'] = pd.to_datetime(hotel_bookings['reservation_status_date'])

"""## Missing Value Handling"""

hotel_bookings_clean = hotel_bookings.copy()

"""The dataset structure shows missing values in several columns: `children` (4), `country` (488), `agent` (16340), and `company` (112593).

* `children`: The small number of missing values here suggests that simply removing the rows with missing values might be a viable option, as it would have minimal impact on the overall dataset size.

  Handling: **rows removal**

* `country`: 488 missing values is a relatively small number compared to the total rows. Thus, removal of these rows is an appropriate option.

  Handling: **rows removal**

* `agent` and `company`: These columns have a very large number of missing values. The company column, with over 112,000 missing values out of 119,390 rows, is almost entirely empty. These columns are suitable for removal from the analysis.

  Handling: **columns removal**
"""

# Remove rows with empty values in 'children' and 'country' columns
hotel_bookings_clean = hotel_bookings_clean.dropna(subset=['children', 'country'])

# Remove column agent and company
hotel_bookings_clean = hotel_bookings_clean.drop(['agent', 'company'], axis=1)

"""## Outliers Handling

The minimum and maximum values reveal potential outliers in several numerical columns namely `adr`, `adults`, `children`, `babies`.

### `adr` with negative value and highest value

`adr`: The minimum value of -6.38 and a maximum value of 5400.0 are highly suspicious. A negative ADR is likely an error and should be investigated or removed. An ADR of 5400.0 seems unusually high and could be an outlier that skews statistical analysis and impacts model training.
"""

plt.figure(figsize=(8, 6))
sns.boxplot(x=hotel_bookings_clean['adr'])
plt.title('Box Plot of ADR')
plt.show()

"""The box plot illustrates that the `adr` distribution is heavily skewed to the right, with many bookings having a relatively low ADR, but a significant number of bookings with much higher rates, appearing as outliers. The negative ADR value(s) are also highlighted as extreme outliers."""

hotel_bookings_clean[hotel_bookings_clean['adr'] < 0]

hotel_bookings_clean.nlargest(10, 'adr')

"""`adr` outliers handling:

*   Removal: remove rows with negative ADR because The presence of negative adr values is highly likely to be a data error.
*   Winsorizing: replace highest row (ADR = 5400.00) with the nearest non-outlier value. This reduces the influence of extremes without removing the data points entirely.
"""

hotel_bookings_clean = hotel_bookings_clean[hotel_bookings_clean['adr'] >= 0]

# Find the second highest ADR value
second_highest_adr = hotel_bookings_clean['adr'].nlargest(2).iloc[-1]

# Find the index of the row with the highest ADR value
highest_adr_index = hotel_bookings_clean['adr'].idxmax()

# Replace the highest ADR value with the second highest ADR value
hotel_bookings_clean.loc[highest_adr_index, 'adr'] = second_highest_adr

hotel_bookings_clean.nlargest(10, 'adr')

"""`adults`, `children`, `babies`: Maximum values like 55, 10, and 10 seem unusually high for the number of occupants in typical hotel rooms and could represent outliers or data entry errors."""

# Create the box plots
plt.figure(figsize=(12, 6))

plt.subplot(1, 3, 1)
sns.boxplot(y=hotel_bookings_clean['adults'])
plt.title('Adults')

plt.subplot(1, 3, 2)
sns.boxplot(y=hotel_bookings_clean['children'])
plt.title('Children')

plt.subplot(1, 3, 3)
sns.boxplot(y=hotel_bookings_clean['babies'])
plt.title('Babies')

plt.tight_layout()
plt.show()

"""### zero `adults`"""

# Filtering cases
filter1 = hotel_bookings_clean[(hotel_bookings_clean['adults'] == 0) &
                               (hotel_bookings_clean['children'] == 0) &
                               (hotel_bookings_clean['babies'] == 0)]

filter2 = hotel_bookings_clean[(hotel_bookings_clean['adults'] == 0) &
                               (hotel_bookings_clean['children'] > 0) &
                               (hotel_bookings_clean['babies'] > 0)]

# Columns to visualize
columns_to_plot = [
    'reservation_status',
    'market_segment',
    'distribution_channel',
    'customer_type',
    'required_car_parking_spaces'
]

# Create subplots
fig, axes = plt.subplots(len(columns_to_plot), 2, figsize=(16, 20))
fig.subplots_adjust(hspace=0.5)

# Generate bar charts
for idx, col in enumerate(columns_to_plot):
    # First column (filter1)
    counts1 = filter1[col].value_counts().sort_index()
    axes[idx, 0].bar(counts1.index.astype(str), counts1.values, color='skyblue')
    axes[idx, 0].set_title(f'{col} (Adults=0, Children=0, Babies=0)')
    axes[idx, 0].set_ylabel('Count')
    for i, v in enumerate(counts1.values):
        axes[idx, 0].text(i, v, str(v), ha='center')

    # Second column (filter2)
    counts2 = filter2[col].value_counts().sort_index()
    axes[idx, 1].bar(counts2.index.astype(str), counts2.values, color='salmon')
    axes[idx, 1].set_title(f'{col} (Adults=0, Children>0, Babies>0)')
    for i, v in enumerate(counts2.values):
        axes[idx, 1].text(i, v, str(v), ha='center')

plt.tight_layout()
plt.show()

"""**Case 1** (Adults=0, Children=0, Babies=0)
* **Unrealistic combinations**: No guests at all, yet shows completed check-outs and some parking spots.
* **Possible Causes**: These appear to be test, placeholder, or system-error entries.

**Case 2** (Adults=0, Children>0, Babies>0)
* Just 3 bookings with 2 “Check-Out” and 1 “No-Show”.
* **Possible Causes**: Data-entry mistakes (adult count missed) or Edge cases (school/child-only booking with adult info lost).

**Handling**: Remove rows with `total_guests` = `adults` + `children` + `babies` == 0
"""

# Create the 'total_guests' column
hotel_bookings_clean['total_guests'] = (
    hotel_bookings_clean['adults'] +
    hotel_bookings_clean['children'] +
    hotel_bookings_clean['babies']
)

# Remove rows where total_guests == 0
hotel_bookings_clean = hotel_bookings_clean[hotel_bookings_clean['total_guests'] > 0]

"""### high numbers of `adults`"""

# Calculate statistics
mean_adults = hotel_bookings_clean['adults'].mean()
median_adults = hotel_bookings_clean['adults'].median()
percentiles_adults = hotel_bookings_clean['adults'].quantile([0.01, 0.05, 0.10, 0.25, 0.5, 0.75, 0.90, 0.95, 0.99])

# Display results
print(f"Mean number of adults: {mean_adults:.2f}")
print(f"Median number of adults: {median_adults}")
print("\nPercentiles of adults:")
print(percentiles_adults)

"""Descriptive statistics shows:
* The mean and median number of adults are low (1.86 and 2.0 respectively), indicating that most bookings are for a small number of adults.
* The percentiles reinforce this, showing that 75% of bookings have 2 adults, and even the 99th percentile is only 3 adults. This means bookings with 4 or more adults are relatively rare occurrences (outliers), falling into the top 1% of the data distribution.
"""

# Calculate the number of bookings in each category
adults_lt_4 = len(hotel_bookings_clean[hotel_bookings_clean['adults'] < 4])
adults_4_10 = len(hotel_bookings_clean[(hotel_bookings_clean['adults'] >= 4) & (hotel_bookings_clean['adults'] <= 10)])
adults_11_20 = len(hotel_bookings_clean[(hotel_bookings_clean['adults'] > 10) & (hotel_bookings_clean['adults'] <= 20)])
adults_21_30 = len(hotel_bookings_clean[(hotel_bookings_clean['adults'] > 20) & (hotel_bookings_clean['adults'] <= 30)])
adults_31_40 = len(hotel_bookings_clean[(hotel_bookings_clean['adults'] > 30) & (hotel_bookings_clean['adults'] <= 40)])
adults_41_50 = len(hotel_bookings_clean[(hotel_bookings_clean['adults'] > 40) & (hotel_bookings_clean['adults'] <= 50)])
adults_gt_50 = len(hotel_bookings_clean[hotel_bookings_clean['adults'] > 50])

total_bookings = len(hotel_bookings_clean)

# Calculate the percentages
percentage_lt_4 = (adults_lt_4 / total_bookings) * 100
percentage_4_10 = (adults_4_10 / total_bookings) * 100
percentage_11_20 = (adults_11_20 / total_bookings) * 100
percentage_21_30 = (adults_21_30 / total_bookings) * 100
percentage_31_40 = (adults_31_40 / total_bookings) * 100
percentage_41_50 = (adults_41_50 / total_bookings) * 100
percentage_gt_50 = (adults_gt_50 / total_bookings) * 100

# Print the results
print("Number of Bookings and Percentages:")
print(f"- Adults < 4: {adults_lt_4} ({percentage_lt_4:.5f}%)")
print(f"- Adults >= 4 and <= 10: {adults_4_10} ({percentage_4_10:.5f}%)")
print(f"- Adults > 10 and <= 20: {adults_11_20} ({percentage_11_20:.5f}%)")
print(f"- Adults > 20 and <= 30: {adults_21_30} ({percentage_21_30:.5f}%)")
print(f"- Adults > 30 and <= 40: {adults_31_40} ({percentage_31_40:.5f}%)")
print(f"- Adults > 40 and <= 50: {adults_41_50} ({percentage_41_50:.5f}%)")
print(f"- Adults > 50: {adults_gt_50} ({percentage_gt_50:.5f}%)")

"""**Handling**:
* Remove bookings with adults > 10 due to rare occurences
* Keep bookings with adults from 4 to 10 to consider a higher numbers of adults without being overly sensitive to the exact, infrequent high counts
"""

hotel_bookings_clean = hotel_bookings_clean[hotel_bookings_clean['adults'] <= 10]

"""## Univariate Analysis

Identifies feature types in the dataset based on data type
"""

# Identify feature types
time_features = hotel_bookings_clean.select_dtypes(include=['datetime64']).columns.tolist()
numerical_features = hotel_bookings_clean.select_dtypes(include=np.number).columns.tolist()
categorical_features = hotel_bookings_clean.select_dtypes(include='object').columns.tolist()
categorical_features = [feature for feature in categorical_features if feature != 'country']
geolocation_features = ['country']

print(f"Numerical Features ({len(numerical_features)}): {numerical_features}")
print(f"Categorical Features ({len(categorical_features)}): {categorical_features}")
print(f"Geolocation Features ({len(geolocation_features)}): {geolocation_features}")
print(f"Time Features ({len(time_features)}): {time_features}")

"""### Categorical Features"""

for feature in categorical_features:
    count = hotel_bookings_clean[feature].value_counts()
    percent = 100 * hotel_bookings_clean[feature].value_counts(normalize=True)
    df = pd.DataFrame({'Rows': count, '% of Total Rows': percent.round(2)})
    print(f"Feature: {feature}")
    print(df)
    print("\n")

# Define the number of rows and columns for the subplots
n_rows = (len(categorical_features) + 1) // 2  # Calculate the number of rows needed
n_cols = 2

# Create the subplots
fig, axes = plt.subplots(n_rows, n_cols, figsize=(22, 4 * n_rows))

# Flatten the axes array for easier iteration
axes = axes.flatten()

# Iterate through the categorical features and create bar plots
for i, feature in enumerate(categorical_features):
    ax = axes[i]
    plot = sns.countplot(x=feature, data=hotel_bookings_clean, ax=ax)
    ax.set_title(f'Distribution of {feature}')
    ax.set_xlabel(feature)
    ax.set_ylabel('Count')
    plt.setp(ax.get_xticklabels(), ha='center')

    # Add data labels on top of each bar
    for p in plot.patches:
        height = p.get_height()
        ax.annotate(f'{height}',
                    (p.get_x() + p.get_width() / 2, height),
                    ha='center', va='bottom', fontsize=9)

# Remove any unused subplots
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

"""Based on univariate analysis for categorical features, we find that:
* categorical feature distributions shows that there are bins/classes with low bookings number. It is recommended to merged those bins into "Other" class to avoid sparse dummy columns and improve generalization.
* Use of one-hot encoding after class merging.
* `arrival_date_month` can be converted into numerical based on month order.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%markdown
# | Feature                     | Rare-Bookings Handling                         | Encoding Method               | Special Notes                               |
# |-----------------------------|------------------------------------------------|-------------------------------|---------------------------------------------|
# | **hotel**                   | –                                              | Binary (0/1)                  | Two balanced levels (~2 : 1 split)          |
# | **arrival_date_month**      | –                                              | Convert numerical (0-12) | Captures seasonality                        |
# | **meal**                    | Combine “Undefined” & “FB” into “Other”        | One-Hot                       | Four bins: BB, HB, SC, Other                |
# | **market_segment**          | Merge “Complementary” & “Aviation” into “Other”| One-Hot                       |       –
# | **distribution_channel**    | Merge “GDS” & “Undefined” into “Other”         | One-Hot                       | Four bins: TA/TO, Direct, Corporate, Other  |
# | **reserved_room_type**      | Group all except top two (A, D) into “Other”   | One-Hot                       | Reduces sparse levels                       |
# | **assigned_room_type**      | Group all except top two (A, D) into “Other”   | One-Hot                       | Check for mismacthes room type first            |
# | **deposit_type**            | Merge “Refundable” into “Non Refund”/Other     | One-Hot                       | Two or three bins depending on correlation with is_canceled     |
# | **customer_type**           | Merge “Group” into “Other”                     | One-Hot                       | Three bins: Transient, Transient-Party, Other |

"""### Numerical Features"""

hotel_bookings_clean.hist(bins=50, figsize=(20,15))
plt.show()

"""Here is summary of univariate analysis of numerical features"""

# Commented out IPython magic to ensure Python compatibility.
# %%markdown
# | **Feature Name** | **Description** | **Data Preparation** |
# |-----------------------------|------------------------------------------------|-------------------------------|
# | **is_canceled** | Target variable. Shows counts of canceled (1) vs. non-canceled (0) bookings, indicating class balance. | Address class imbalance (e.g., over/under-sampling, SMOTE, class weights). Use appropriate evaluation metrics (Precision, Recall, F1, AUC). |
# | **lead_time** | Right-skewed: Most bookings made close to arrival, fewer far in advance. | Requires scaling. |
# | **arrival_date_year** | Shows booking counts per year (e.g., 2015, 2016, 2017). Indicates data collection period/trends. | Ensure time-aware validation if trends exist. |
# | **arrival_date_week_number** | Distribution of bookings across weeks (1-52/53), showing seasonal peaks and troughs. | Key for seasonality. Consider cyclical encoding (sine/cosine transformation) |
# | **arrival_date_day_of_month** | Fairly uniform distribution of bookings by day of the month (1-31). | Generally used as is. |
# | **stays_in_weekend_nights** | Right-skewed: Most bookings have 0-2 weekend nights; higher counts are rare. | Consider capping rare higher values or feature engineering (e.g., binary has_weekend_stay). Scaling might be needed. |
# | **stays_in_week_nights** | Right-skewed: Most stays are short (1-5 weekday nights); longer stays less common. | Consider log/sqrt transformation for skewness. Handle potential outliers (very long stays). Scaling. Feature engineer total_stay_duration. |
# | **adults** | Most bookings for 1-2 adults. | Consider binning. |
# | **children** | Highly right-skewed & zero-inflated: Vast majority have 0 children. | Consider binary feature (has_children). Scaling if used as is. |
# | **babies** | Extremely right-skewed & zero-inflated: Overwhelmingly 0 babies. | Consider binary feature (has_babies) or assess predictive power due to rarity of non-zero values. |
# | **is_repeated_guest** | Binary (0/1): Most guests are not repeated. | Use as is. Often a good predictor. |
# | **previous_cancellations** | Highly right-skewed & zero-inflated: Most customers have 0 previous cancellations. | Consider log1p transformation for skewness. Capping rare high values. |
# | **previous_bookings_not_canceled** | 	Highly right-skewed & zero-inflated: Most have 0 previous non-canceled bookings (new customers). | Consider log1p transformation. Capping rare high values. |
# | **booking_changes** | Right-skewed & zero-inflated: Most bookings have 0 changes. | Consider log1p transformation or capping. Could create a binary "changes_made" feature. |
# | **days_in_waiting_list** | Extremely right-skewed & zero-inflated: Vast majority have 0 days on waiting list. | Consider binary feature (was_on_waiting_list). Assess predictive power if non-zero instances are very rare. |
# | **adr** | Somewhat right-skewed or normal-like. Potential for values near/at zero. | Requires scaling. |
# | **required_car_parking_spaces** | Extremely right-skewed & zero-inflated: Overwhelmingly 0 spaces requested. | Convert to binary feature (parking_requested). Exact number beyond 0 or 1 often too sparse. |
# | **total_of_special_requests** | Right-skewed: Most bookings have 0 special requests, then 1, 2, etc. | Consider capping if high numbers are rare. A binary "has_special_requests" feature might be useful. |
# | **reservation_status_date** | Distribution of dates when the reservation's final status was recorded. Spans a date range. | Crucial Feature Engineering: Extract year, month, day. Calculate time differences (e.g., time from booking to this date). |
# | **total_guests** | Distribution of total guests (adults+children+babies) | Often a good engineered feature. Scaling |

"""## Multivariate Analysis

### Categorical Features
"""

# Create subplots
fig, axes = plt.subplots(len(categorical_features), 2, figsize=(24, 4 * len(categorical_features)))
fig.subplots_adjust(hspace=0.5)

# Generate bar charts
for idx, col in enumerate(categorical_features):
    # First column (filtered by 'is_canceled' == 0)
    # Get counts and sort by value (largest to smallest)
    counts_no = hotel_bookings_clean[hotel_bookings_clean['is_canceled'] == 0][col].value_counts()
    # Use the index of the sorted counts for plotting
    axes[idx, 0].bar(counts_no.index.astype(str), counts_no.values, color='skyblue')
    axes[idx, 0].set_title(f'{col} (is_canceled = 0)')
    axes[idx, 0].set_ylabel('Count')
    axes[idx, 0].tick_params(axis='x')
    # Add text labels on top of bars
    for i, v in enumerate(counts_no.values):
        axes[idx, 0].text(i, v, str(v), ha='center')

    # Second column (filtered by 'is_canceled' == 1)
    # Get counts for canceled bookings and sort by value (largest to smallest)
    counts_canceled = hotel_bookings_clean[hotel_bookings_clean['is_canceled'] == 1][col].value_counts()
    # Use the index of the sorted counts for plotting
    axes[idx, 1].bar(counts_canceled.index.astype(str), counts_canceled.values, color='salmon')
    axes[idx, 1].set_title(f'{col} (is_canceled = 1)')
    axes[idx, 1].tick_params(axis='x')
    # Add text labels on top of bars
    for i, v in enumerate(counts_canceled.values):
        axes[idx, 1].text(i, v, str(v), ha='center')

plt.tight_layout()
plt.show()

"""Based bar graphs for each categorical features, we can find that:
* Only `deposit_type` that shows visually significant difference between canceled bookings and non-canceled bookings.
* bookings with `is_canceled` equal 1 has two `reservation_status`(Canceled or No Show). This shows redudancy between `is_canceled` and `reservation_status`

We can also evaluate the difference within categorical features with Chi-Squared test.
"""

from scipy.stats import chi2_contingency

# Perform Chi-Square test for each categorical feature
for feature in categorical_features:
    contingency_table = pd.crosstab(hotel_bookings_clean['is_canceled'], hotel_bookings_clean[feature])
    chi2, p, dof, expected = chi2_contingency(contingency_table)
    print(f"Chi-Square Test for {feature}:")
    print(f"Chi2 statistic: {chi2:.3f}")
    print(f"P-value: {p:.3f}")
    print(f"Degrees of freedom: {dof}")
    if p < 0.05:
        print("Reject the null hypothesis. There is a significant difference between is_canceled = 0 and is_canceled = 1.")
    print("\n")

"""Based on chi-square test, it seems that all categorical features has significant difference if the bookings are canceled or not. Therefor, other method is needed for feature selection.

### Numerical Features
"""

# Correlation Matrix
plt.figure(figsize=(12, 10))
sns.heatmap(hotel_bookings_clean[numerical_features].corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Numerical Features')
plt.show()

"""Because `is_canceled` can be treated as numerical_features, correlation test can be conducted to evaluate significance correlation between numerical feautres and `is_canceled`"""

from scipy.stats import pearsonr

correlation_results = []
for feature in numerical_features:
    if feature != 'is_canceled':  # Exclude the target variable from self-correlation
        correlation, p_value = pearsonr(hotel_bookings_clean['is_canceled'], hotel_bookings_clean[feature])
        if p_value < 0.05:
            significance = 'Significant'
        else:
            significance = 'Not Significant'
        correlation_results.append([feature, correlation, p_value, significance])

correlation_df = pd.DataFrame(correlation_results, columns=['Feature', 'Correlation', 'P-value', 'Significance'])
print(correlation_df)

"""Based on correlation matrix and correlation significance test (numerical features vs `is_canceled`), we can find that:

`lead_time`
* **Correlation with is_canceled:** 0.291 (P-value: 0.0000e+00, Significant). A moderate positive correlation that is highly statistically significant.
* **Impact on Feature Selection:** Strong candidate for inclusion. The significance confirms this isn't a chance finding.

`arrival_date_year`
* **Correlation with is_canceled:** 0.017 (P-value: 9.9339e-09, Significant). Although the correlation is very weak, it is statistically significant.
* **Impact on Feature Selection:** While significant, the practical impact of such a small correlation might be minimal. It could contribute minorly to the model, but its multicollinearity with `arrival_date_week_number` (-0.54) needs consideration. Could be tested, but also a candidate for dropping if it doesn't improve model performance or if simpler models are preferred.

`arrival_date_week_number`
* **Correlation with is_canceled:** 0.008 (P-value: 9.3308e-03, Significant). Very weak positive correlation, but statistically significant.
* **Impact on Feature Selection:** Similar to `arrival_date_year`, the significance of a very weak correlation means there's likely a real (but small) linear effect. Could be tested, potentially engineered (e.g., for seasonality using cyclical encoding), or dropped if it adds little value.

`arrival_date_day_of_month`
* **Correlation with is_canceled:** -0.006 (P-value: 4.0296e-02, Significant). Very weak negative correlation, statistically significant (just under the 0.05 threshold).
* **Impact on Feature Selection:** The significance suggests a real, albeit tiny, relationship. Likely has minimal predictive power on its own. Could be a candidate for dropping.

`stays_in_weekend_nights`
* **Correlation with is_canceled:** -0.002 (P-value: 0.4301691, Not Significant). Effectively no linear correlation, and this lack of correlation is confirmed by the high p-value.
* **Impact on Feature Selection:** Strong candidate for removal. It doesn't have a statistically significant linear relationship with the target.

`stays_in_week_nights`
* **Correlation with is_canceled:** 0.025 (P-value: 1.5848e-17, Significant). A weak positive correlation, but highly statistically significant.
* **Impact on Feature Selection:** While weak, its significance suggests it might carry some information. Could be combined with `stays_in_weekend_nights` (which is not significant on its own) into `total_stay_duration` and then tested. The multicollinearity (0.49) with `stays_in_weekend_nights` is also a factor.

`adults`
* **Correlation with is_canceled:** 0.059 (P-value: 1.4919e-91, Significant). Weak positive correlation, but highly statistically significant.
* **Impact on Feature Selection:** The significance is strong. However, its high correlation with `total_guests` (0.77) suggests potential redundancy if `total_guests` is used. The decision would depend on whether the individual count of adults offers more predictive nuance than the combined `total_guests`.

`children`
* **Correlation with is_canceled:** 0.005 (P-value: 0.1155424, Not Significant). Very weak positive correlation, and it is not statistically significant.
* **Impact on Feature Selection:** Strong candidate for removal, especially given its lack of significance and correlation with `total_guests` (0.68).

`babies`
* **Correlation with is_canceled:** -0.033 (P-value: 2.7604e-29, Significant). Weak negative correlation, but highly statistically significant.
* **Impact on Feature Selection:** The significance suggests it might hold some predictive value, despite the small correlation magnitude. Its correlation with `total_guests` (0.65) is also a consideration.

`is_repeated_guest`
* **Correlation with is_canceled:** -0.084 (P-value: 2.9402e-185, Significant). Weak to moderate negative correlation, highly statistically significant.
* **Impact on Feature Selection:** Good candidate for inclusion. Its correlation with `previous_bookings_not_canceled` (0.43) means you might test using one or the other if multicollinearity is an issue, but both being significant is interesting.

`previous_cancellations`
* **Correlation with is_canceled:** 0.110 (P-value: 7.8711e-316, Significant). Weak to moderate positive correlation, extremely statistically significant.
* **Impact on Feature Selection:** Good candidate for inclusion.

`previous_bookings_not_canceled`
* **Correlation with is_canceled:** -0.055 (P-value: 1.3461e-81, Significant). Weak negative correlation, highly statistically significant.
* **Impact on Feature Selection:** Likely include, despite the weaker correlation, due to high significance. Consider its relationship with `is_repeated_guest`.

`booking_changes`
* **Correlation with is_canceled:** -0.145 (P-value: 0.0000e+00, Significant). Moderate negative correlation, highly statistically significant.
* **Impact on Feature Selection:** Strong candidate for inclusion.

`days_in_waiting_list`
* **Correlation with is_canceled:** 0.054 (P-value: 9.4771e-78, Significant). Weak positive correlation, but highly statistically significant.
* **Impact on Feature Selection:** Its significance suggests it's not just noise. Could be included and its contribution evaluated.

`adr` (Average Daily Rate)
* **Correlation with is_canceled:** 0.046 (P-value: 2.6181e-57, Significant). Weak positive correlation, highly statistically significant.
* **Impact on Feature Selection:** Even though the linear correlation is weak, its high significance suggests a reliable (though small) linear relationship. It's worth keeping and exploring further, perhaps for non-linear effects or interactions, especially given its business relevance.

`required_car_parking_spaces`
* **Correlation with is_canceled:** -0.195 (P-value: 0.0000e+00, Significant). Moderate negative correlation, highly statistically significant.
* **Impact on Feature Selection:** Strong candidate for inclusion.

`total_of_special_requests`
* **Correlation with is_canceled:** -0.236 (P-value: 0.0000e+00, Significant). Moderate negative correlation, highly statistically significant.
* **Impact on Feature Selection:** Strong candidate for inclusion.

`total_guests`
* **Correlation with is_canceled:** 0.042 (P-value: 4.5291e-47, Significant). Weak positive correlation, highly statistically significant.
* **Impact on Feature Selection:** While the direct linear correlation with cancellation is weak, its significance is high. Using `total_guests` instead of `adults`, `children` (not significant), and `babies` (significant but weak correlation) could be a good strategy to handle multicollinearity and capture the overall guest count effect. The model should be tested with `total_guests` versus a combination of its significant components.

### Geolocation Feature
"""

# Select relevant columns
hotel_bookings_subset = hotel_bookings_clean[['is_canceled', 'country']]

# Display the new dataframe
hotel_bookings_subset.head()

def country_to_continent(country_name):
  try:
    country_alpha2 = pc.country_name_to_country_alpha2(country_name)
    country_continent_code = pc.country_alpha2_to_continent_code(country_alpha2)
    country_continent_name = pc.convert_continent_code_to_continent_name(country_continent_code)
    return country_continent_name
  except:
    return "Unknown"

hotel_bookings_clean['continent'] = hotel_bookings_clean['country'].apply(country_to_continent)

continent_list = hotel_bookings_clean['continent'].unique()
continent_list

# Filter rows with 'Unknown' continent
unknown_continent_df = hotel_bookings_clean[hotel_bookings_clean['continent'] == 'Unknown']

# Identify countries with 'Unknown' continent
unknown_countries = unknown_continent_df['country'].unique()

print("Countries with Unknown Continent:")
unknown_countries

# Change continent value using loc function
hotel_bookings_clean.loc[hotel_bookings_clean['country'] == 'CN', 'continent'] = 'Asia'
hotel_bookings_clean.loc[hotel_bookings_clean['country'] == 'TMP', 'continent'] = 'Asia'
hotel_bookings_clean.loc[hotel_bookings_clean['country'] == 'UMI', 'continent'] = 'North America'
hotel_bookings_clean.loc[hotel_bookings_clean['country'] == 'ATA', 'continent'] = 'South America'
hotel_bookings_clean.loc[hotel_bookings_clean['country'] == 'ATF', 'continent'] = 'South America'

# Filter rows with 'Unknown' continent
unknown_continent_df = hotel_bookings_clean[hotel_bookings_clean['continent'] == 'Unknown']

# Identify countries with 'Unknown' continent
unknown_countries = unknown_continent_df['country'].unique()

print("Countries with Unknown Continent:")
unknown_countries

bookings_by_country = hotel_bookings_clean[hotel_bookings_clean['is_canceled'] == 0].groupby('country')['is_canceled'].count().reset_index()
bookings_by_country.rename(columns={'is_canceled': 'booking_count'}, inplace=True)
total_bookings = bookings_by_country['booking_count'].sum()
bookings_by_country['percentage'] = (bookings_by_country['booking_count'] / total_bookings) * 100
bookings_by_country = bookings_by_country.sort_values(by='booking_count', ascending=False)

bookings_by_country

bookings_by_country = hotel_bookings_clean[hotel_bookings_clean['is_canceled'] == 1].groupby('country')['is_canceled'].count().reset_index()
bookings_by_country.rename(columns={'is_canceled': 'booking_count'}, inplace=True)
total_bookings = bookings_by_country['booking_count'].sum()
bookings_by_country['percentage'] = (bookings_by_country['booking_count'] / total_bookings) * 100
bookings_by_country = bookings_by_country.sort_values(by='booking_count', ascending=False)

bookings_by_country

top_10_countries = bookings_by_country.head(10)

plt.figure(figsize=(12, 6))
bars = plt.bar(top_10_countries['country'], top_10_countries['booking_count'])
plt.xlabel("Country")
plt.ylabel("Count of Bookings")
plt.title("Top 10 Countries by Booking Count")
plt.xticks(ha="center")  # Rotate x-axis labels for better readability


# Add data labels (count and percentage) on top of each bar
for bar, count, percentage in zip(bars, top_10_countries['booking_count'], top_10_countries['percentage']):
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval, f'{int(count)} ({percentage:.1f}%)', ha='center', va='bottom')

plt.tight_layout()
plt.show()

bookings_by_continent = hotel_bookings_clean[hotel_bookings_clean['is_canceled'] == 0].groupby('continent')['is_canceled'].count().reset_index()
bookings_by_continent.rename(columns={'is_canceled': 'booking_count'}, inplace=True)
total_bookings = bookings_by_continent['booking_count'].sum()
bookings_by_continent['percentage'] = (bookings_by_continent['booking_count'] / total_bookings) * 100
bookings_by_continent = bookings_by_continent.sort_values(by='booking_count', ascending=False)

bookings_by_continent

bookings_by_continent = hotel_bookings_clean[hotel_bookings_clean['is_canceled'] == 1].groupby('continent')['is_canceled'].count().reset_index()
bookings_by_continent.rename(columns={'is_canceled': 'booking_count'}, inplace=True)
total_bookings = bookings_by_continent['booking_count'].sum()
bookings_by_continent['percentage'] = (bookings_by_continent['booking_count'] / total_bookings) * 100
bookings_by_continent = bookings_by_continent.sort_values(by='booking_count', ascending=False)

bookings_by_continent

continent_bookings = hotel_bookings_clean['continent'].value_counts()

plt.figure(figsize=(12, 6))
bars = plt.bar(continent_bookings.index, continent_bookings.values)
plt.xlabel("Continent")
plt.ylabel("Number of Bookings")
plt.title("Number of Bookings per Continent")

# Add data labels above the bars
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval, int(yval), ha='center', va='bottom')

plt.show()

hotel_bookings_clean['PRT'] = 0  # Initialize the new column with 0
hotel_bookings_clean.loc[hotel_bookings_clean['country'] == 'PRT', 'PRT'] = 1

from scipy.stats import pearsonr

correlation, p_value = pearsonr(hotel_bookings_clean['PRT'], hotel_bookings_clean['is_canceled'])

print(f"Correlation between PRT and is_canceled: {correlation}")
print(f"P-value: {p_value}")

if p_value < 0.05:
  print("The correlation is statistically significant.")
else:
  print("The correlation is not statistically significant.")

"""## Summary of EDA

![Sample Image](https://raw.githubusercontent.com/eru2024/laskarai-mlt-predictiveanalytics/master/img/summary_eda.jpg)

# Data Preparation

Data prep1

Original Features:

* `is_canceled`
* `hotel`: binary encoding
* `arrival_date_month`: convert numerical
* `meal`: one-hot encoding (BB, HB, SC, Other)
* `market_segment`: one-hot encoding (Online TA, Offline TA/TO, Direct, Corporate, Other)
* `reserved_room_type`: one-hot encoding (A, D, Other)
*  `deposit_type`: one-hot encoding (No Deposit, Refundable, Non Refund)
* `customer_type`: one-hot encoding (Transient, Transient-Party, Other)
* `lead_time`: scaling
* `arrival_date_week_number`: as-is
* `arrival_date_day_of_month`: as-is
* `previous_cancellations`: as-is
* `previous_bookings_not_canceled`: as-is
* `booking_changes`: as-is
* `days_in_waiting_list`: as-is
* `adr`: scaling
* `required_car_parking_spaces`: as-is
* `total_of_special_requests`: as-is

New Features:
* `match_room_type`: binary encoding (0: `reserved_room_type` != `assigned_room_type`, 1: `reserved_room_type` == `assigned_room_type`)
* `has_weekend_stay`: binary encoding (0: `stays_in_weekend_nights` = 0, 1: `stays_in_weekend_nights` > 0)
* `total_stay_duration`: `stays_in_weekend_nights` + `stays_in_week_nights`
* `has_babies`: binary encoding (0: `babies` = 0, 1: `babies` > 0)
* `total_guests` (already included in `hotel_bookings_clean): as-is
* `PRT`: binary encoding (0: `country` != 'PRT', 1: `country` == 'PRT')
"""

hotel_bookings_prep1 = hotel_bookings_clean[[
    'is_canceled', 'hotel', 'arrival_date_month', 'meal',
    'market_segment', 'reserved_room_type', 'deposit_type',
    'customer_type', 'lead_time', 'arrival_date_week_number',
    'arrival_date_day_of_month', 'previous_cancellations',
    'previous_bookings_not_canceled', 'booking_changes',
    'days_in_waiting_list', 'adr', 'required_car_parking_spaces',
    'total_of_special_requests'
]].copy()

print(hotel_bookings_prep1.head())

"""## New Features

`match_room_type`: binary encoding (0: `reserved_room_type` != `assigned_room_type`, 1: `reserved_room_type` == `assigned_room_type`)
"""

# Create the 'match_room_type' column
hotel_bookings_prep1['match_room_type'] = np.where(
    hotel_bookings_clean['reserved_room_type'] == hotel_bookings_clean['assigned_room_type'],
    1, 0
)

"""`has_weekend_stay`: binary encoding (0: `stays_in_weekend_nights` = 0, 1: `stays_in_weekend_nights` > 0)"""

# Create the 'has_weekend_stay' column
hotel_bookings_prep1['has_weekend_stay'] = np.where(
    hotel_bookings_clean['stays_in_weekend_nights'] > 0, 1, 0
)

"""`total_stay_duration`: `stays_in_weekend_nights` + `stays_in_week_nights`"""

hotel_bookings_prep1['total_stay_duration'] = (
    hotel_bookings_clean['stays_in_weekend_nights'] +
    hotel_bookings_clean['stays_in_week_nights']
)

"""`has_babies`: binary encoding (0: `babies` = 0, 1: `babies` > 0)"""

# `has_babies`: binary encoding (0: `babies` = 0, 1: `babies` > 0)
hotel_bookings_prep1['has_babies'] = np.where(
    hotel_bookings_clean['babies'] > 0, 1, 0
)

"""`total_guests` (already included in `hotel_bookings_clean): as-is"""

hotel_bookings_prep1['total_guests'] = hotel_bookings_clean['total_guests']

"""`PRT`: binary encoding (0: `country` != 'PRT', 1: `country` == 'PRT')"""

hotel_bookings_prep1['PRT'] = 0  # Initialize the new column with 0
hotel_bookings_prep1.loc[hotel_bookings_clean['country'] == 'PRT', 'PRT'] = 1

print(hotel_bookings_prep1.head())

"""## Scaling"""

from scipy.stats import normaltest

# Assuming hotel_bookings_prep1 is already loaded and defined

# Perform the normality test for 'lead_time'
lead_time_stat, lead_time_p = normaltest(hotel_bookings_prep1['lead_time'])
print(f"Lead Time Normality Test:")
print(f"Statistic: {lead_time_stat:.3f}")
print(f"P-value: {lead_time_p:.3f}")

if lead_time_p < 0.05:
    print("Reject the null hypothesis: Lead time does not follow a normal distribution.")
else:
    print("Fail to reject the null hypothesis: Lead time may follow a normal distribution.")

print("\n")

# Perform the normality test for 'adr'
adr_stat, adr_p = normaltest(hotel_bookings_prep1['adr'])
print(f"ADR Normality Test:")
print(f"Statistic: {adr_stat:.3f}")
print(f"P-value: {adr_p:.3f}")

if adr_p < 0.05:
    print("Reject the null hypothesis: ADR does not follow a normal distribution.")
else:
    print("Fail to reject the null hypothesis: ADR may follow a normal distribution.")

"""Because `lead_time` and `adr` doesn't follow normal distribution, normalization is used for scaling"""

from sklearn.preprocessing import MinMaxScaler

# Create a MinMaxScaler object
scaler = MinMaxScaler()

# Fit and transform the 'lead_time' column
hotel_bookings_prep1['lead_time_norm'] = scaler.fit_transform(hotel_bookings_prep1[['lead_time']])

# Fit and transform the 'adr' column
hotel_bookings_prep1['adr_norm'] = scaler.fit_transform(hotel_bookings_prep1[['adr']])

print(hotel_bookings_prep1.head())

"""## Encoding

`hotel`: binary encoding
"""

# Create binary encoding for the 'hotel' column
hotel_bookings_prep1['hotel_City Hotel'] = (hotel_bookings_prep1['hotel'] == 'City Hotel').astype(int)
hotel_bookings_prep1['hotel_Resort Hotel'] = (hotel_bookings_prep1['hotel'] == 'Resort Hotel').astype(int)

hotel_bookings_prep1[['hotel','hotel_City Hotel','hotel_Resort Hotel']].head()

"""`arrival_date_month`: convert numerical"""

# Create a dictionary to map month names to numbers
month_mapping = {
    'January': 1, 'February': 2, 'March': 3, 'April': 4, 'May': 5, 'June': 6,
    'July': 7, 'August': 8, 'September': 9, 'October': 10, 'November': 11, 'December': 12
}

# Convert 'arrival_date_month' to numerical values using the mapping
hotel_bookings_prep1['arrival_date_month_number'] = hotel_bookings_prep1['arrival_date_month'].map(month_mapping)

# Display the original and new columns
print(hotel_bookings_prep1[['arrival_date_month', 'arrival_date_month_number']].head())

"""`meal`: one-hot encoding (BB, HB, SC, Others)"""

print(hotel_bookings_prep1['meal'].unique())

# Replace 'FB' and 'Undefined' with 'Others' in the 'meal' column
hotel_bookings_prep1['meal'] = hotel_bookings_prep1['meal'].replace(['FB', 'Undefined'], 'Others')

# One-hot encode the 'meal' column
meal_dummies = pd.get_dummies(hotel_bookings_prep1['meal'], prefix='meal')
hotel_bookings_prep1 = pd.concat([hotel_bookings_prep1, meal_dummies], axis=1)
print(hotel_bookings_prep1['meal'].unique())

# Display the encoded columns along with the original 'meal' column
print(hotel_bookings_prep1[['meal', 'meal_BB', 'meal_HB', 'meal_SC', 'meal_Others']].head())

# Convert encoded columns to binary
for col in ['meal_BB', 'meal_HB', 'meal_SC', 'meal_Others']:
    hotel_bookings_prep1[col] = hotel_bookings_prep1[col].astype(int)

print(hotel_bookings_prep1[['meal', 'meal_BB', 'meal_HB', 'meal_SC', 'meal_Others']].head())

"""`market_segment`: one-hot encoding (Online TA, Offline TA/TO, Direct, Corporate, Others)"""

print(hotel_bookings_prep1['market_segment'].unique())

# Create 'Others'
hotel_bookings_prep1['market_segment'] = hotel_bookings_prep1['market_segment'].replace(['Complementary', 'Groups', 'Aviation'], 'Others')

# One-hot encode
dummies = pd.get_dummies(hotel_bookings_prep1['market_segment'], prefix='market_segment')
hotel_bookings_prep1 = pd.concat([hotel_bookings_prep1, dummies], axis=1)
print(hotel_bookings_prep1['market_segment'].unique())

# Display the encoded columns along with the original column
print(hotel_bookings_prep1[['market_segment',
                            'market_segment_Online TA',
                            'market_segment_Offline TA/TO',
                            'market_segment_Direct',
                            'market_segment_Corporate',
                            'market_segment_Others']].head())

# Convert encoded columns to binary
for col in ['market_segment_Online TA',
                            'market_segment_Offline TA/TO',
                            'market_segment_Direct',
                            'market_segment_Corporate',
                            'market_segment_Others']:
    hotel_bookings_prep1[col] = hotel_bookings_prep1[col].astype(int)

print(hotel_bookings_prep1[['market_segment',
                            'market_segment_Online TA',
                            'market_segment_Offline TA/TO',
                            'market_segment_Direct',
                            'market_segment_Corporate',
                            'market_segment_Others']].head())

"""`reserved_room_type`: one-hot encoding (A, D, Othesr)"""

print(hotel_bookings_prep1['reserved_room_type'].unique())

# Replace values other than 'A' and 'D' with 'Others'
hotel_bookings_prep1['reserved_room_type'] = hotel_bookings_prep1['reserved_room_type'].apply(lambda x: x if x in ['A', 'D'] else 'Others')

# One-hot encode the 'reserved_room_type' column
reserved_room_type_dummies = pd.get_dummies(hotel_bookings_prep1['reserved_room_type'], prefix='reserved_room_type')
hotel_bookings_prep1 = pd.concat([hotel_bookings_prep1, reserved_room_type_dummies], axis=1)

print(hotel_bookings_prep1[['reserved_room_type', 'reserved_room_type_A', 'reserved_room_type_D', 'reserved_room_type_Others']].head())

# Convert encoded columns to binary
for col in ['reserved_room_type_A', 'reserved_room_type_D', 'reserved_room_type_Others']:
    hotel_bookings_prep1[col] = hotel_bookings_prep1[col].astype(int)

print(hotel_bookings_prep1[['reserved_room_type', 'reserved_room_type_A', 'reserved_room_type_D', 'reserved_room_type_Others']].head())

"""`deposit_type`: one-hot encoding (No Deposit, Refundable, Non Refund)"""

print(hotel_bookings_prep1['deposit_type'].unique())

# One-hot encode the 'deposit_type' column
deposit_type_dummies = pd.get_dummies(hotel_bookings_prep1['deposit_type'], prefix='deposit_type')
hotel_bookings_prep1 = pd.concat([hotel_bookings_prep1, deposit_type_dummies], axis=1)

print(hotel_bookings_prep1[['deposit_type', 'deposit_type_No Deposit', 'deposit_type_Non Refund', 'deposit_type_Refundable']].head())

# Convert encoded columns to binary
for col in ['deposit_type_No Deposit', 'deposit_type_Non Refund', 'deposit_type_Refundable']:
    hotel_bookings_prep1[col] = hotel_bookings_prep1[col].astype(int)

print(hotel_bookings_prep1[['deposit_type', 'deposit_type_No Deposit', 'deposit_type_Non Refund', 'deposit_type_Refundable']].head())

"""`customer_type`: one-hot encoding (Transient, Transient-Party, Others)"""

print(hotel_bookings_prep1['customer_type'].unique())

# Replace values other than 'Transient' and 'Transient-Party' with 'Others'
hotel_bookings_prep1['customer_type'] = hotel_bookings_prep1['customer_type'].apply(lambda x: x if x in ['Transient', 'Transient-Party'] else 'Others')

# One-hot encode the 'customer_type' column
customer_type_dummies = pd.get_dummies(hotel_bookings_prep1['customer_type'], prefix='customer_type')
hotel_bookings_prep1 = pd.concat([hotel_bookings_prep1, customer_type_dummies], axis=1)

print(hotel_bookings_prep1[['customer_type', 'customer_type_Transient', 'customer_type_Transient-Party', 'customer_type_Others']].head())

# Convert encoded columns to binary
for col in ['customer_type_Transient', 'customer_type_Transient-Party', 'customer_type_Others']:
    hotel_bookings_prep1[col] = hotel_bookings_prep1[col].astype(int)

print(hotel_bookings_prep1[['customer_type', 'customer_type_Transient', 'customer_type_Transient-Party', 'customer_type_Others']].head())

"""Preparing new dataframe for modeling"""

# Drop object data type columns
object_columns = hotel_bookings_prep1.select_dtypes(include=['object']).columns
hotel_bookings_prep1 = hotel_bookings_prep1.drop(columns=object_columns)
hotel_bookings_prep1 = hotel_bookings_prep1.drop(columns=['lead_time','adr'])

dataframes = {'hotel_bookings_prep1': hotel_bookings_prep1}

summary_dataset = get_dataframe_summary(dataframes)
summary_dataset

"""# Model Development

## SMOTE - Data Splitting

Applies SMOTE to balance the `is_canceled` target variable by oversampling the minority class.
"""

from imblearn.over_sampling import SMOTE

# Separate features (X) and target (y)
X = hotel_bookings_prep1.drop('is_canceled', axis=1)
y = hotel_bookings_prep1['is_canceled']

# Initialize SMOTE
smote = SMOTE(random_state=42)

# Apply SMOTE to oversample the minority class
X_resampled, y_resampled = smote.fit_resample(X, y)

# Create a new DataFrame with the resampled data
hotel_bookings_prep1_resampled = pd.DataFrame(X_resampled, columns=X.columns)
hotel_bookings_prep1_resampled['is_canceled'] = y_resampled

"""Splits the resampled dataset into training (90%) and testing (10%) sets."""

from sklearn.model_selection import train_test_split

X_train_80, X_test_80, y_train_80, y_test_80 = train_test_split(X_resampled, y_resampled, test_size = 0.1, random_state = 123)

print(f'Total # of sample in whole dataset: {len(X)}')
print(f'Total # of sample in train dataset: {len(X_train_80)}')
print(f'Total # of sample in test dataset: {len(X_test_80)}')

"""## Logistic Regression"""

logreg_model = make_pipeline(StandardScaler(), LogisticRegression())
logreg_model.fit(X_train_80, y_train_80)

# Make predictions on the test set
y_pred_80 = logreg_model.predict(X_test_80)

# Evaluate the model
accuracy = accuracy_score(y_test_80, y_pred_80)
print(f"Accuracy: {accuracy}")
print(classification_report(y_test_80, y_pred_80))

# Create and display the confusion matrix
cm = confusion_matrix(y_test_80, y_pred_80)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Not Canceled', 'Canceled'],
            yticklabels=['Not Canceled', 'Canceled'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""## K-Nearest Neighbor (KNN)"""

from sklearn.neighbors import KNeighborsRegressor

knn_model = KNeighborsRegressor(n_neighbors=3)
knn_model.fit(X_train_80, y_train_80)

# Make predictions on the test set
y_pred_80 = knn_model.predict(X_test_80)

y_pred_80 = np.where(y_pred_80 >= 0.5, 1, 0)

# Evaluate the model
accuracy = accuracy_score(y_test_80, y_pred_80)
print(f"Accuracy: {accuracy}")
print(classification_report(y_test_80, y_pred_80))

# Create and display the confusion matrix
cm = confusion_matrix(y_test_80, y_pred_80)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Not Canceled', 'Canceled'],
            yticklabels=['Not Canceled', 'Canceled'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""## Decision Tree Classifier"""

from sklearn.tree import DecisionTreeClassifier

# Initialize and train the Decision Tree Classifier
dt_classifier = DecisionTreeClassifier(random_state=42)
dt_classifier.fit(X_train_80, y_train_80)

# Make predictions on the test set
y_pred_80 = dt_classifier.predict(X_test_80)

# Evaluate the model
accuracy = accuracy_score(y_test_80, y_pred_80)
print(f"Accuracy: {accuracy}")
print(classification_report(y_test_80, y_pred_80))

# Create and display the confusion matrix
cm = confusion_matrix(y_test_80, y_pred_80)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Not Canceled', 'Canceled'],
            yticklabels=['Not Canceled', 'Canceled'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""## Random Forest"""

from sklearn.ensemble import RandomForestClassifier

# Initialize and train the Random Forest Classifier
rf_classifier = RandomForestClassifier(random_state=42)
rf_classifier.fit(X_train_80, y_train_80)

# Make predictions on the test set
y_pred_80 = rf_classifier.predict(X_test_80)

# Evaluate the model
accuracy = accuracy_score(y_test_80, y_pred_80)
print(f"Accuracy: {accuracy}")
print(classification_report(y_test_80, y_pred_80))

# Create and display the confusion matrix
cm = confusion_matrix(y_test_80, y_pred_80)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Not Canceled', 'Canceled'],
            yticklabels=['Not Canceled', 'Canceled'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""## Adaptive Boosting Classifier"""

from sklearn.ensemble import AdaBoostClassifier

# Initialize and train the AdaBoost Classifier
ada_classifier = AdaBoostClassifier(random_state=42)
ada_classifier.fit(X_train_80, y_train_80)

# Make predictions on the test set
y_pred_80 = ada_classifier.predict(X_test_80)

# Evaluate the model
accuracy = accuracy_score(y_test_80, y_pred_80)
print(f"Accuracy: {accuracy}")
print(classification_report(y_test_80, y_pred_80))

# Create and display the confusion matrix
cm = confusion_matrix(y_test_80, y_pred_80)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Not Canceled', 'Canceled'],
            yticklabels=['Not Canceled', 'Canceled'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""## Gradient Boosting Classifier"""

import matplotlib.pyplot as plt
from sklearn.ensemble import GradientBoostingClassifier

# Initialize and train the Gradient Boosting Classifier
gb_classifier = GradientBoostingClassifier(random_state=42)
gb_classifier.fit(X_train_80, y_train_80)

# Make predictions on the test set
y_pred_80 = gb_classifier.predict(X_test_80)

# Evaluate the model
accuracy = accuracy_score(y_test_80, y_pred_80)
print(f"Accuracy: {accuracy}")
print(classification_report(y_test_80, y_pred_80))

# Create and display the confusion matrix
cm = confusion_matrix(y_test_80, y_pred_80)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Not Canceled', 'Canceled'],
            yticklabels=['Not Canceled', 'Canceled'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""# Model Evaluation

## Best Algorithm
"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Assuming y_test_80 and y_pred_80 are defined for each model as in the provided code

models = {
    'Logistic Regression': logreg_model,
    'KNN': knn_model,
    'Decision Tree': dt_classifier,
    'Random Forest': rf_classifier,
    'AdaBoost': ada_classifier,
    'Gradient Boosting': gb_classifier
}

results = []
for model_name, model in models.items():
    if model_name == 'KNN':
        y_pred_80 = model.predict(X_test_80)
        y_pred_80 = np.where(y_pred_80 >= 0.5, 1, 0)
    else:
        y_pred_80 = model.predict(X_test_80)

    accuracy = accuracy_score(y_test_80, y_pred_80)
    precision = precision_score(y_test_80, y_pred_80)
    recall = recall_score(y_test_80, y_pred_80)
    f1 = f1_score(y_test_80, y_pred_80)

    results.append([model_name, accuracy, precision, recall, f1])

summary_df = pd.DataFrame(results, columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score'])
summary_df

"""Comparing six models using Accuracy, Precision, Recall, and F1-Score, Random Forest achieves the highest performance across all metrics, especially F1-Score (0.905), making it the best model for predicting booking cancellations.

## Feature Importance

Calculates feature importance using permutation importance on the trained Random Forest model, based on F1-Score. Results are sorted by mean importance to identify the most influential features.
"""

from sklearn.inspection import permutation_importance

result = permutation_importance(rf_classifier, X_test_80, y_test_80, scoring='f1', n_repeats=10, random_state=42)
perm_importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance Mean': result.importances_mean,
    'Importance Std': result.importances_std
}).sort_values(by='Importance Mean', ascending=False)

print(perm_importance)

"""It is clear that top 3 features are `PRT`, `lead_time`, and ` total_of_special_requests` based on importance mean.

Adds predicted cancellation probabilities and class labels from the trained Random Forest model to `hotel_bookings_pred` using feature set `X`.
"""

hotel_bookings_pred = hotel_bookings_clean.copy()

# Predict using the trained RandomForestClassifier
hotel_bookings_pred['prob_canceled'] = rf_classifier.predict_proba(X)[:, 1]
hotel_bookings_pred['pred_canceled'] = rf_classifier.predict(X)

"""### `PRT`"""

# Calculate the mean prob_canceled for each PRT group
mean_prob_canceled_by_prt = hotel_bookings_pred.groupby('PRT')['prob_canceled'].mean()

# Create the bar graph
plt.figure(figsize=(8, 6))
bars = plt.bar(mean_prob_canceled_by_prt.index, mean_prob_canceled_by_prt.values)

# Add data labels on top of each bar
for bar in bars:
    height = bar.get_height()
    plt.text(
        bar.get_x() + bar.get_width() / 2,  # x-position
        height,                            # y-position
        f"{height:.2f}",                   # label text
        ha='center', va='bottom'           # alignment
    )

plt.xlabel("PRT")
plt.ylabel("Mean Probability of Cancellation")
plt.title("Mean Probability of Cancellation based on PRT")
plt.xticks(mean_prob_canceled_by_prt.index)
plt.ylim(0, max(mean_prob_canceled_by_prt.values) + 0.05)  # Add space above bars for labels
plt.show()

"""This binary feature (1: country is Portugal, 0: other) had the highest importance mean at 0.0736. Analysis of booking behavior shows a clear pattern: bookings from Portugal have a significantly higher cancellation probability (57.5%) compared to those from other countries (25.4%). This strong contrast in behavior makes the PRT feature a key predictor in the model.

### `lead_time`
"""

# Calculate the mean of prob_canceled for each lead_time
mean_prob_canceled_by_lead_time = hotel_bookings_pred.groupby('lead_time')['prob_canceled'].mean().reset_index()

# Create the scatter plot with regression line
plt.figure(figsize=(10, 6))
sns.regplot(x='lead_time', y='prob_canceled', data=mean_prob_canceled_by_lead_time, scatter_kws={'s': 20})
plt.xlabel('Lead Time')
plt.ylabel('Mean of prob_canceled')
plt.title('Scatter Plot of Lead Time vs. Mean of prob_canceled')
plt.show()

"""With an importance mean of 0.0715, the normalized lead time between the booking date and check-in date proves to be a strong indicator of cancellation likelihood. The longer the lead time, the higher the chance of cancellation. Cancellations increase steadily from 37.3% in the 4–104 day interval to 81.8% in the 504–604 day range, eventually reaching 100% in the longest lead time range (604–704 days). This trend highlights the risk of long-term bookings and emphasizes the need for closer monitoring or flexible policies for such cases.

### `total_of_special_requests`
"""

# Calculate the mean of prob_canceled
mean_prob_canceled_by_total_of_special_requests = hotel_bookings_pred.groupby('total_of_special_requests')['prob_canceled'].mean().reset_index()

# Create the scatter plot with regression line
plt.figure(figsize=(10, 6))
sns.regplot(x='total_of_special_requests', y='prob_canceled', data=mean_prob_canceled_by_total_of_special_requests, scatter_kws={'s': 20})
plt.xlabel('Total of Special Requests')
plt.ylabel('Mean of prob_canceled')
plt.title('Scatter Plot of Total of Special Requests vs. Mean of prob_canceled')
plt.show()

"""This feature, with an importance mean of 0.0686, reflects the level of customer engagement through requests made during booking. The data shows that bookings with no special requests have a cancellation rate of 49.1%, while those with five special requests drop sharply to only 7.6%. This pattern suggests that guests who invest effort in customizing their stay are more likely to follow through with their bookings."""